{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U deepsparse-nightly[llm] boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and save the model\n",
    "\n",
    "You can find more models here: [https://sparsezoo.neuralmagic.com/?modelSet=generative_ai&tasks=text_generation](https://sparsezoo.neuralmagic.com/?modelSet=generative_ai&tasks=text_generation)  \n",
    "Or you can optimize your own model through [this pipeline](https://github.com/luis5tb/neural-magic-poc/blob/main/openshift-ai/pipeline_simplified.py) (requires GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparsezoo import Model\n",
    "\n",
    "model_name = \"mpt-7b-dolly_mpt_pretrain-pruned50_quantized\"\n",
    "\n",
    "model = Model(\"zoo:{model_name}\")\n",
    "model.download()\n",
    "print(model.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "region_name = os.environ.get('AWS_DEFAULT_REGION')\n",
    "bucket_name = os.environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n",
    "                                aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "s3_resource = session.resource(\n",
    "    's3',\n",
    "    config=botocore.client.Config(signature_version='s3v4'),\n",
    "    endpoint_url=endpoint_url,\n",
    "    region_name=region_name)\n",
    "\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "#upload the model directory without git or safetensors\n",
    "\n",
    "def upload_directory_to_s3(local_directory, s3_prefix, remove_safetensors=False):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(file_path, local_directory)\n",
    "            if \".git\" in relative_path:\n",
    "                print(f\"skipping {relative_path}\")\n",
    "                continue\n",
    "            if remove_safetensors and \".safetensors\" in relative_path:\n",
    "                print(f\"skipping {relative_path}\")\n",
    "                continue\n",
    "            s3_key = os.path.join(s3_prefix, relative_path)\n",
    "            print(f\"{file_path} -> {s3_key}\")\n",
    "            bucket.upload_file(file_path, s3_key)\n",
    "\n",
    "\n",
    "def list_objects(prefix):\n",
    "    filter = bucket.objects.filter(Prefix=prefix)\n",
    "    for obj in filter.all():\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_directory_to_s3(model.path, f\"models/{model_name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
